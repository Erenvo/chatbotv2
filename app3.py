# --- START OF FILE app2.py ---

import streamlit as st
# from openai import OpenAI # EÄŸer farklÄ± bir LLM saÄŸlayÄ±cÄ± kullanacaksanÄ±z
import os
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings # VEYA kullandÄ±ÄŸÄ±nÄ±z diÄŸer LLM/Embedding kÃ¼tÃ¼phaneleri
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
# from langchain_core.messages import HumanMessage, SystemMessage # EÄŸer mesaj objeleriyle Ã§alÄ±ÅŸacaksanÄ±z
import traceback
import uuid
import requests
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS # YENÄ°: DuckDuckGo arama iÃ§in
# import trafilatura

# -----------------------------------------------------------------------------
# SAYFA KONFÄ°GÃœRASYONU
# -----------------------------------------------------------------------------
st.set_page_config(page_title="Ã‡ok KaynaklÄ± AI AsistanÄ±", page_icon="ðŸŒ")
# -----------------------------------------------------------------------------

# os.environ["TOKENIZERS_PARALLELISM"] = "false" # HuggingFace yerel modeller iÃ§in

# --- API KonfigÃ¼rasyonu (Ã–rnek: Google AI) ---
GOOGLE_API_KEY = st.secrets.get("GOOGLE_API_KEY")
GOOGLE_LLM_MODEL_NAME = st.secrets.get("GOOGLE_LLM_MODEL_NAME", "gemini-1.5-flash-latest")
GOOGLE_EMBEDDING_MODEL_NAME = st.secrets.get("GOOGLE_EMBEDDING_MODEL_NAME", "models/embedding-001")

if not GOOGLE_API_KEY:
    st.error("Google API anahtarÄ± bulunamadÄ±! LÃ¼tfen Streamlit Secrets bÃ¶lÃ¼mÃ¼nÃ¼ kontrol edin.")
    st.stop()

try:
    llm_client = ChatGoogleGenerativeAI(
        model=GOOGLE_LLM_MODEL_NAME,
        google_api_key=GOOGLE_API_KEY,
        temperature=0.2, # Ã–zetleme iÃ§in biraz daha yÃ¼ksek olabilir
        # safety_settings={ ... }
    )
    embeddings_model_global = GoogleGenerativeAIEmbeddings(
        model=GOOGLE_EMBEDDING_MODEL_NAME,
        google_api_key=GOOGLE_API_KEY
    )
    print("Google AI istemcileri baÅŸarÄ±yla baÄŸlandÄ±.")
except Exception as e:
    st.error(f"AI istemcileri oluÅŸturulurken hata: {e}")
    st.error(traceback.format_exc())
    st.stop()

# --- Web Sitesi Ä°Ã§eriÄŸi Ã‡ekme Fonksiyonu ---
def get_website_text(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        # YENÄ°: Ä°Ã§erik tipini kontrol et (isteÄŸe baÄŸlÄ± ama iyi bir pratik)
        content_type = response.headers.get('content-type', '').lower()
        if 'text/html' not in content_type:
            # st.warning(f"URL '{url}' HTML iÃ§eriÄŸi dÃ¶ndÃ¼rmedi (Tip: {content_type}). AtlanÄ±yor.")
            print(f"URL '{url}' HTML iÃ§eriÄŸi dÃ¶ndÃ¼rmedi (Tip: {content_type}). AtlanÄ±yor.")
            return "" # Veya None, iÅŸleme mantÄ±ÄŸÄ±nÄ±za gÃ¶re

        soup = BeautifulSoup(response.content, 'lxml')
        for script_or_style in soup(["script", "style", "header", "footer", "nav", "aside", "form", "noscript", "iframe", "button", "select", "input", "img", "svg", "link", "meta"]): # Daha fazla gereksiz etiket
            script_or_style.decompose()
        
        body = soup.find('body')
        if body:
            # text_nodes = body.find_all(string=True)
            # visible_text = ""
            # for t_node in text_nodes:
            #     parent = t_node.parent
            #     if parent.name not in ['style', 'script', 'head', 'title', 'meta', '[document]'] and not isinstance(t_node, Comment):
            #         stripped_text = t_node.strip()
            #         if stripped_text:
            #             visible_text += stripped_text + "\n"
            # cleaned_text = "\n".join([line.strip() for line in visible_text.splitlines() if line.strip()])
            
            # Daha iyi bir metin Ã§Ä±karma yÃ¶ntemi:
            paragraphs = body.find_all(['p', 'div', 'span', 'article', 'section', 'td', 'li']) # Daha fazla potansiyel metin iÃ§eren etiket
            cleaned_text = ""
            for tag in paragraphs:
                text_content = tag.get_text(separator=' ', strip=True)
                if text_content: # Sadece anlamlÄ± iÃ§eriÄŸi olanlarÄ± ekle
                    # KÄ±sa, anlamsÄ±z stringleri filtrele (Ã¶rneÄŸin, sadece navigasyon linkleri vb.)
                    # Bu kÄ±sÄ±m daha da geliÅŸtirilebilir.
                    if len(text_content.split()) > 3: # En az 3 kelime varsa al gibi bir basit kural
                         cleaned_text += text_content + "\n\n" # Paragraflar arasÄ±na boÅŸluk

            # Ã‡oklu boÅŸluklarÄ± ve satÄ±rlarÄ± tek bir taneye indirge (son bir temizlik)
            cleaned_text = "\n".join([line.strip() for line in cleaned_text.splitlines() if line.strip()])
            return cleaned_text
        return ""
    except requests.exceptions.RequestException as e:
        st.warning(f"Web sitesi iÃ§eriÄŸi Ã§ekilirken hata: {url} - {e}") # st.error yerine warning
        return "" # BoÅŸ string dÃ¶ndÃ¼r, akÄ±ÅŸÄ± kesmesin
    except Exception as e:
        st.warning(f"Web sitesi iÃ§eriÄŸi iÅŸlenirken beklenmedik hata: {url} - {e}") # st.error yerine warning
        return "" # BoÅŸ string dÃ¶ndÃ¼r

# YENÄ°: Web Arama Fonksiyonu
def search_web_for_query(query, num_results=3):
    """
    Verilen sorgu iÃ§in web'de arama yapar ve ilk N sonucun URL'lerini dÃ¶ndÃ¼rÃ¼r.
    """
    results = []
    try:
        with DDGS() as ddgs:
            search_results = list(ddgs.text(query, region='wt-wt', safesearch='moderate', max_results=num_results*2)) # Biraz fazla alÄ±p filtreleyelim
            # Sadece geÃ§erli URL'leri ve kÄ±sa olanlarÄ± filtrele (Ã¶rneÄŸin PDF, DOC linkleri deÄŸil)
            count = 0
            for r in search_results:
                if r.get('href') and r['href'].startswith('http') and \
                   not any(ext in r['href'].lower() for ext in ['.pdf', '.doc', '.docx', '.ppt', '.pptx', '.xls', '.xlsx', '.zip', '.rar']):
                    results.append(r['href'])
                    count += 1
                    if count >= num_results:
                        break
            if not results and search_results: # HiÃ§ uygun URL bulunamadÄ±ysa ilkini almayÄ± dene
                if search_results[0].get('href'):
                     results.append(search_results[0]['href'])
    except Exception as e:
        st.error(f"Web aramasÄ± sÄ±rasÄ±nda hata: {e}")
    return results

# YENÄ°: Web Ä°Ã§eriÄŸi Ã–zetleme iÃ§in Prompt Åžablonu
def get_web_summary_prompt_template():
    prompt_template_str = """
    SENÄ°N GÃ–REVÄ°N: KullanÄ±cÄ±nÄ±n bir sorgusu Ã¼zerine web'den aÅŸaÄŸÄ±daki "BaÄŸlam:" bÃ¶lÃ¼mÃ¼nde Ã§eÅŸitli sayfalardan bilgiler toplandÄ±.
    Bu bilgileri kullanarak, kullanÄ±cÄ±nÄ±n orijinal sorgusuna yanÄ±t veren, anlaÅŸÄ±lÄ±r, tarafsÄ±z ve kapsamlÄ± bir Ã¶zet oluÅŸtur.
    Ã–zetin SADECE saÄŸlanan "BaÄŸlam:" iÃ§indeki bilgilere dayanmalÄ±dÄ±r.
    EÄŸer saÄŸlanan baÄŸlam yetersizse veya konuyla alakasÄ±zsa, "SaÄŸlanan web sayfalarÄ±ndan bu sorgu iÃ§in yeterli bilgi Ã§Ä±karÄ±lamadÄ±." yanÄ±tÄ±nÄ± ver.
    YanÄ±tÄ±nÄ± doÄŸrudan Ã¶zetle baÅŸlat.

    BaÄŸlam:
    {context}

    KullanÄ±cÄ±nÄ±n Orijinal Sorgusu: {original_query}

    Ã–zet Cevap:"""
    return PromptTemplate(template=prompt_template_str, input_variables=["context", "original_query"])

# --- DiÄŸer YardÄ±mcÄ± Fonksiyonlar ---
def get_pdf_text(pdf_docs):
    text = ""
    if pdf_docs:
        for pdf in pdf_docs:
            try:
                pdf_reader = PdfReader(pdf)
                for page in pdf_reader.pages:
                    page_text = page.extract_text()
                    if page_text: text += page_text
            except Exception as e: st.warning(f"'{pdf.name}' dosyasÄ±ndan metin Ã§Ä±karÄ±lÄ±rken hata: {e}")
    return text

def get_text_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500, 
        chunk_overlap=250,
        length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )
    return text_splitter.split_text(text)

def create_vector_store_from_chunks(text_chunks, current_embeddings_model):
    if not text_chunks or not current_embeddings_model:
        return None
    try:
        return FAISS.from_texts(texts=text_chunks, embedding=current_embeddings_model)
    except Exception as e:
        st.error(f"VektÃ¶r deposu oluÅŸturulurken hata: {e}"); st.error(traceback.format_exc()); return None

def get_conversational_chain_prompt_template():
    prompt_template_str = """
    SENÄ°N GÃ–REVÄ°N: Sen, kullanÄ±cÄ± tarafÄ±ndan saÄŸlanan bir metin kaynaÄŸÄ±ndaki (bu bir PDF veya bir web sitesi iÃ§eriÄŸi olabilir) bilgilere dayanarak sorularÄ± yanÄ±tlayan son derece dikkatli ve kuralcÄ± bir AI asistanÄ±sÄ±n. Temel amacÄ±n, kullanÄ±cÄ±nÄ±n sorularÄ±na SADECE ve YALNIZCA aÅŸaÄŸÄ±da "BaÄŸlam:" olarak belirtilen metin iÃ§eriÄŸinden yararlanarak cevap vermektir.

    UYMAN GEREKEN KESÄ°N KURALLAR:
    1.  BAÄžLAM DIÅžINA Ã‡IKMA: Kendi genel bilgini, internetten eriÅŸebileceÄŸin diÄŸer bilgileri veya daha Ã¶nceki sohbetlerden edindiÄŸin bilgileri KESÄ°NLÄ°KLE yanÄ±tlarÄ±na dahil etme. Senin bilgi evrenin SADECE o anki "BaÄŸlam:" ile sÄ±nÄ±rlÄ±dÄ±r.
    2.  YORUMSUZ AKTARIM: "BaÄŸlam:" iÃ§indeki bilgileri olduÄŸu gibi veya anlamÄ±nÄ± deÄŸiÅŸtirmeden aktar. Kendi yorumlarÄ±nÄ±, Ã§Ä±karÄ±mlarÄ±nÄ±, kiÅŸisel gÃ¶rÃ¼ÅŸlerini veya eklemelerini ASLA yapma.
    3.  BÄ°LGÄ° YOKSA NET DURUÅž: EÄŸer kullanÄ±cÄ±nÄ±n sorusunun cevabÄ± "BaÄŸlam:" iÃ§inde AÃ‡IKÃ‡A bulunmuyorsa veya "BaÄŸlam:" bÃ¶lÃ¼mÃ¼ boÅŸ ise, TEREDDÃœT ETMEDEN ve SADECE ÅŸu yanÄ±tÄ± ver:
        "Bu bilgi saÄŸlanan kaynakta (PDF/Web Sitesi) bulunmuyor."
    4.  "BÄ°LGÄ° YOK" MESAJININ KISITLARI: YukarÄ±daki "bilgi yok" mesajÄ±nÄ± verdikten sonra, ASLA ek bir aÃ§Ä±klama, Ã¶zÃ¼r, varsayÄ±m, yÃ¶nlendirme veya "ama genel olarak..." gibi ifadeler kullanma. CevabÄ±n SADECE bu standart mesajdan ibaret olmalÄ±dÄ±r.
    5.  Ã–ZETLEME Ä°STEKLERÄ°: EÄŸer kullanÄ±cÄ± kaynaÄŸÄ±n genel bir Ã¶zetini, ana konusunu, ne anlattÄ±ÄŸÄ±nÄ± veya benzeri bir genel bakÄ±ÅŸ istiyorsa ("Ã¶zetle", "ne hakkÄ±nda", "konusu ne" gibi ifadelerle):
        a.  "BaÄŸlam:" olarak sana sunulan (bu durumda genellikle kaynaÄŸÄ±n tamamÄ± veya bÃ¼yÃ¼k bir kÄ±smÄ± olacaktÄ±r) metindeki ana fikirleri, kilit noktalarÄ± ve Ã¶nemli detaylarÄ± dikkatlice analiz et.
        b.  Bu analiz sonucunda, kaynaÄŸÄ±n genelini yansÄ±tan, kapsamlÄ± ama mÃ¼mkÃ¼n olduÄŸunca Ã¶z bir Ã¶zet oluÅŸtur.
        c.  Bu Ã¶zeti oluÅŸtururken de KESÄ°NLÄ°KLE "BaÄŸlam:" dÄ±ÅŸÄ±na Ã§Ä±kma. Sadece baÄŸlamdaki bilgileri kullanarak Ã¶zet yap.
    6.  DOÄžRUDAN VE NET OL: CevaplarÄ±n aÃ§Ä±k, anlaÅŸÄ±lÄ±r ve doÄŸrudan sorulan soruyla ilgili olsun. Gereksiz uzun giriÅŸlerden veya dolaylÄ± anlatÄ±mlardan kaÃ§Ä±n.

    BaÄŸlam:
    {context}

    Soru: {question}

    Cevap:"""
    return PromptTemplate(template=prompt_template_str, input_variables=["context", "question"])

# --- Session State ve Oturum YÃ¶netimi ---
if "sessions" not in st.session_state: st.session_state.sessions = {}
if "current_session_id" not in st.session_state: st.session_state.current_session_id = None
if "prompt_template" not in st.session_state: st.session_state.prompt_template = get_conversational_chain_prompt_template()
if "web_summary_prompt_template" not in st.session_state: st.session_state.web_summary_prompt_template = get_web_summary_prompt_template() # YENÄ°

def create_new_session(session_type="pdf"):
    session_id = str(uuid.uuid4())
    type_prefix = "PDF Sohbeti" if session_type == "pdf" else "Web Sohbeti"
    session_name = f"{type_prefix} {len(st.session_state.sessions) + 1}"
    st.session_state.sessions[session_id] = {
        "id": session_id, "name": session_name,
        "source_type": session_type,
        "source_info": None, 
        "vector_store": None, "chat_history": [], "processed": False,
        "full_text_for_summary": None 
    }
    st.session_state.current_session_id = session_id
    return session_id

def get_active_session_data():
    if st.session_state.current_session_id and st.session_state.current_session_id in st.session_state.sessions:
        return st.session_state.sessions[st.session_state.current_session_id]
    return None

def delete_session(session_id_to_delete):
    if session_id_to_delete in st.session_state.sessions:
        del st.session_state.sessions[session_id_to_delete]
        if st.session_state.current_session_id == session_id_to_delete:
            st.session_state.current_session_id = None
            if st.session_state.sessions: st.session_state.current_session_id = list(st.session_state.sessions.keys())[0]

st.title("ðŸŒ Ã‡ok KaynaklÄ± AI AsistanÄ±")

# --- Kenar Ã‡ubuÄŸu ---
with st.sidebar:
    st.header("Sohbet YÃ¶netimi")
    col1, col2 = st.columns(2)
    with col1:
        if st.button("âž• Yeni PDF Sohbeti", key="new_pdf_chat", use_container_width=True): create_new_session(session_type="pdf"); st.rerun()
    with col2:
        if st.button("ðŸ”— Yeni Web Sohbeti", key="new_web_chat", use_container_width=True): create_new_session(session_type="website"); st.rerun()

    session_options = {
        sid: f"{sdata['name']} ({sdata.get('source_info', 'BoÅŸ') if isinstance(sdata.get('source_info'), str) else ', '.join(sdata.get('source_info', [])) if sdata.get('source_info') else 'BoÅŸ'})"
        for sid, sdata in st.session_state.sessions.items()
    }

    if not session_options and st.session_state.current_session_id is None:
        create_new_session(); st.rerun() # VarsayÄ±lan olarak bir PDF sohbeti oluÅŸturabilir veya kullanÄ±cÄ±ya seÃ§tirebilir.

    if session_options:
        current_index = 0
        if st.session_state.current_session_id in session_options:
            current_index = list(session_options.keys()).index(st.session_state.current_session_id)
        
        selected_session_id = st.selectbox(
            "Aktif Sohbet:", options=list(session_options.keys()),
            format_func=lambda sid: session_options.get(sid, "Bilinmeyen"),
            index=current_index,
            key="session_selector"
        )
        if selected_session_id != st.session_state.current_session_id:
            st.session_state.current_session_id = selected_session_id; st.rerun()

        active_session = get_active_session_data()
        if active_session:
            st.markdown("---"); st.subheader(f"DÃ¼zenle: {active_session['name']}")
            raw_text_from_source = None

            if active_session["source_type"] == "pdf":
                uploader_key = f"pdf_uploader_{active_session['id']}"
                uploaded_files = st.file_uploader("PDF dosyalarÄ±:", accept_multiple_files=True, type="pdf", key=uploader_key, label_visibility="collapsed")
                if st.button("PDF'leri Ä°ÅŸle", key=f"process_pdf_{active_session['id']}", use_container_width=True):
                    if uploaded_files:
                        active_session["source_info"] = [f.name for f in uploaded_files]
                        raw_text_from_source = get_pdf_text(uploaded_files)
                    else: st.warning("LÃ¼tfen PDF dosyasÄ± yÃ¼kleyin.")
            
            elif active_session["source_type"] == "website":
                url_input_key = f"url_input_{active_session['id']}"
                website_url = st.text_input("Web sitesi URL'si:", key=url_input_key, value=active_session.get("source_info",""), placeholder="https://ornek.com/sayfa", label_visibility="collapsed")
                if st.button("Web Sitesini Ä°ÅŸle", key=f"process_web_{active_session['id']}", use_container_width=True):
                    if website_url and website_url.startswith(("http://", "https://")):
                        active_session["source_info"] = website_url
                        with st.spinner(f"Ä°Ã§erik Ã§ekiliyor..."):
                            raw_text_from_source = get_website_text(website_url)
                    elif website_url: st.warning("LÃ¼tfen geÃ§erli bir URL girin.")
                    else: st.warning("LÃ¼tfen bir URL girin.")

            if raw_text_from_source is not None:
                with st.spinner("Kaynak iÅŸleniyor..."):
                    active_session["full_text_for_summary"] = raw_text_from_source
                    active_session["chat_history"] = []
                    active_session["vector_store"] = None
                    
                    if not raw_text_from_source.strip():
                        st.error("Kaynak boÅŸ veya metin Ã§Ä±karÄ±lamadÄ±.")
                        active_session["processed"] = False; active_session["full_text_for_summary"] = None
                    else:
                        text_chunks = get_text_chunks(raw_text_from_source)
                        if not text_chunks:
                            st.error("Metin parÃ§alara ayrÄ±lamadÄ±."); active_session["processed"] = False
                        else:
                            vector_store = create_vector_store_from_chunks(text_chunks, embeddings_model_global)
                            if vector_store:
                                active_session["vector_store"] = vector_store; active_session["processed"] = True
                                st.success(f"Kaynak baÅŸarÄ±yla iÅŸlendi."); st.rerun()
                            else:
                                st.error("VektÃ¶r deposu oluÅŸturulamadÄ±."); active_session["processed"] = False
            
            if active_session.get("processed") and active_session.get("source_info"):
                 source_display = active_session["source_info"]
                 if isinstance(source_display, list): source_display = ", ".join(source_display)
                 st.markdown(f"**Ä°ÅŸlenen Kaynak:**")
                 st.caption(f"{source_display}")


            st.markdown("---")
            if st.button(f"'{active_session['name']}' Oturumunu Sil", type="secondary", key=f"delete_btn_{active_session['id']}", use_container_width=True):
                delete_session(active_session['id']); st.success(f"Oturum silindi."); st.rerun()
    else:
        st.sidebar.info("HenÃ¼z bir sohbet oturumu yok.")

# --- Ana Sohbet AlanÄ± ---
active_session_data = get_active_session_data()
if active_session_data:
    st.header(f"Sohbet: {active_session_data['name']}")
    current_source_info = active_session_data.get("source_info")
    if current_source_info:
        source_display = current_source_info
        if isinstance(source_display, list): source_display = ", ".join(source_display)
        st.caption(f"Mevcut Kaynak: {source_display}")
    # YENÄ°: Web arama komutu iÃ§in bilgi
    # else:
    #     st.caption("Bu oturum iÃ§in henÃ¼z bir kaynak iÅŸlenmedi. Web'de arama yapmak iÃ§in 'ara: [sorgunuz]' yazabilirsiniz.")
    
    # YENÄ°: Web arama tetikleyici
    WEB_SEARCH_TRIGGER = "ara:" 
    # veya st.checkbox("Web'de Ara") gibi bir UI elemanÄ± da kullanabilirsiniz.

    for message in active_session_data["chat_history"]:
        with st.chat_message(message["role"]): st.markdown(message["content"])

    if user_query := st.chat_input(f"Kaynak hakkÄ±nda soru sorun veya '{WEB_SEARCH_TRIGGER} [sorgunuz]' ile web'de arayÄ±n..."):
        active_session_data["chat_history"].append({"role": "user", "content": user_query})
        with st.chat_message("user"): st.markdown(user_query)
        with st.chat_message("assistant"):
            message_placeholder = st.empty(); full_response_text = ""
            
            # YENÄ°: Web Arama MantÄ±ÄŸÄ±
            if user_query.lower().startswith(WEB_SEARCH_TRIGGER):
                search_term = user_query[len(WEB_SEARCH_TRIGGER):].strip()
                if not search_term:
                    full_response_text = "LÃ¼tfen aramak istediÄŸiniz konuyu belirtin (Ã¶rneÄŸin: ara: TÃ¼rkiye'nin baÅŸkenti)."
                    message_placeholder.markdown(full_response_text)
                else:
                    message_placeholder.markdown(f"'{search_term}' iÃ§in web'de arama yapÄ±lÄ±yor ve Ã¶zetleniyor...")
                    try:
                        search_urls = search_web_for_query(search_term, num_results=3) # Ä°lk 3 sonucu alalÄ±m
                        if not search_urls:
                            full_response_text = f"'{search_term}' iÃ§in web'de sonuÃ§ bulunamadÄ±."
                        else:
                            st.caption(f"Bulunan kaynaklar: {', '.join(search_urls)}") # DEBUG: Hangi URL'ler bulundu
                            all_web_text = ""
                            MAX_WEB_CONTENT_PER_PAGE = 10000 # Her sayfadan alÄ±nacak max karakter (Ã§ok uzun sayfalardan kaÃ§Ä±nmak iÃ§in)
                            MAX_TOTAL_WEB_CONTENT = 50000 # LLM'e gÃ¶nderilecek toplam max karakter

                            for i, url in enumerate(search_urls):
                                with st.spinner(f"'{url}' adresinden iÃ§erik Ã§ekiliyor... ({i+1}/{len(search_urls)})"):
                                    page_text = get_website_text(url)
                                if page_text:
                                    all_web_text += f"\n\n--- {url} sayfasÄ±ndan iÃ§erik ---\n\n" + page_text[:MAX_WEB_CONTENT_PER_PAGE]
                                if len(all_web_text) > MAX_TOTAL_WEB_CONTENT:
                                    all_web_text = all_web_text[:MAX_TOTAL_WEB_CONTENT] + "\n\n... (iÃ§erik Ã§ok uzundu ve kÄ±saltÄ±ldÄ±)"
                                    break
                            
                            if not all_web_text.strip():
                                full_response_text = "Web sayfalarÄ±ndan anlamlÄ± iÃ§erik Ã§Ä±karÄ±lamadÄ±."
                            else:
                                # DEBUG: st.text_area("Web'den Ã‡ekilen Toplam Metin", all_web_text, height=200)
                                web_summary_prompt = st.session_state.web_summary_prompt_template.format(
                                    context=all_web_text, 
                                    original_query=search_term
                                )
                                # DEBUG: st.text_area("Web Ã–zetleme Prompt'u", web_summary_prompt, height=200)
                                for chunk in llm_client.stream(web_summary_prompt):
                                    if hasattr(chunk, 'content'):
                                        full_response_text += chunk.content
                                        message_placeholder.markdown(full_response_text + "â–Œ")
                                if not full_response_text: # LLM boÅŸ dÃ¶ndÃ¼rÃ¼rse
                                    full_response_text = "SaÄŸlanan web sayfalarÄ±ndan bu sorgu iÃ§in yeterli bilgi Ã§Ä±karÄ±lamadÄ±."
                    except Exception as e:
                        st.error(f"Web aramasÄ± ve Ã¶zetleme sÄ±rasÄ±nda hata: {e}"); st.error(traceback.format_exc())
                        full_response_text = "ÃœzgÃ¼nÃ¼m, web aramasÄ± sÄ±rasÄ±nda bir hata oluÅŸtu."
                    message_placeholder.markdown(full_response_text)
            
            # Mevcut RAG (PDF/Web Sitesi) mantÄ±ÄŸÄ±
            else:
                can_answer_from_source = active_session_data.get("processed", False) and \
                                         (active_session_data.get("vector_store") or active_session_data.get("full_text_for_summary"))
                
                if not can_answer_from_source:
                    full_response_text = "LÃ¼tfen Ã¶nce kenar Ã§ubuÄŸundan bu oturum iÃ§in bir kaynak (PDF/Web Sitesi) yÃ¼kleyip iÅŸleyin veya web'de arama yapmak iÃ§in 'ara: [sorgunuz]' komutunu kullanÄ±n."
                    message_placeholder.markdown(full_response_text)
                else:
                    try:
                        context_text = ""
                        summary_keywords = ["Ã¶zetle", "ne anlatÄ±yor", "konusu ne", "ana fikir", "genel olarak", "genel bakÄ±ÅŸ", "kÄ±saca", "summarize", "what is it about", "main idea", "overview", "gist", "tell me about this document"]
                        is_summary_request = any(keyword in user_query.lower() for keyword in summary_keywords)

                        if is_summary_request and active_session_data.get("full_text_for_summary"):
                            context_text = active_session_data["full_text_for_summary"]
                            MAX_CONTEXT_CHARS = 700000 
                            if len(context_text) > MAX_CONTEXT_CHARS:
                                context_text = context_text[:MAX_CONTEXT_CHARS] + "\n\n... (metin Ã¶zet iÃ§in Ã§ok uzundu ve kÄ±saltÄ±ldÄ±)"
                        
                        elif active_session_data.get("vector_store"): 
                            docs = active_session_data["vector_store"].similarity_search(query=user_query, k=5) 
                            if docs:
                                context_text = "\n\n".join([doc.page_content for doc in docs])
                        
                        if not context_text: 
                            full_response_text = "Bu bilgi saÄŸlanan kaynakta (PDF/Web Sitesi) bulunmuyor."
                        else:
                            current_prompt_template = st.session_state.prompt_template
                            formatted_prompt = current_prompt_template.format(context=context_text, question=user_query)
                            
                            for chunk in llm_client.stream(formatted_prompt):
                                if hasattr(chunk, 'content'):
                                    full_response_text += chunk.content
                                    message_placeholder.markdown(full_response_text + "â–Œ")
                                else: 
                                    print(f"Beklenmedik chunk yapÄ±sÄ±: {chunk}")

                        message_placeholder.markdown(full_response_text)

                    except Exception as e:
                        st.error(f"YanÄ±t alÄ±nÄ±rken bir hata oluÅŸtu: {e}"); st.error(traceback.format_exc())
                        full_response_text = "ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu."; message_placeholder.markdown(full_response_text)
            
            active_session_data["chat_history"].append({"role": "assistant", "content": full_response_text})
else:
    st.info("LÃ¼tfen kenar Ã§ubuÄŸundan bir sohbet seÃ§in veya yeni bir tane baÅŸlatÄ±n.")

st.sidebar.markdown("---")
st.sidebar.caption(f"LLM: {GOOGLE_LLM_MODEL_NAME}")
st.sidebar.caption(f"Embedding: {GOOGLE_EMBEDDING_MODEL_NAME}")
